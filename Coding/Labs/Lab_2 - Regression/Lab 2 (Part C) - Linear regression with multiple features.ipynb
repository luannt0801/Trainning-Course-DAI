{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "delitable": false
   },
   "source": [
    "# Lab 2 (Part C) - Linear regression with multiple features\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "Please complete this Jupyter Notebook file and upload it to blackboard __before 05 February 2020__.\n",
    "</div>\n",
    "\n",
    "In this part of the lab, you will implement linear regression with multiple variables to predict the price of houses. Suppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n",
    "\n",
    "# 1. Loading the dataset\n",
    "The file `housing-dataset.csv` contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house. The following Python code helps you load the dataset from the data file into the variables $X$ and $y$. Read the code and print a small subset of $X$ and $y$ to see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nYou can print a small subset of X and y to see what it looks like.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/housing-dataset.csv\"\n",
    "mydata = np.genfromtxt(filename, delimiter=\",\")\n",
    "\n",
    "# We have n data-points (houses)\n",
    "n = len(mydata)\n",
    "\n",
    "# X is a matrix of two column, i.e. an array of n 2-dimensional data-points\n",
    "X = mydata[:, :2].reshape(n, 2)\n",
    "\n",
    "# y is the vector of outputs, i.e. an array of n scalar values\n",
    "y = mydata[:, -1]\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "You can print a small subset of X and y to see what it looks like.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data normalization\n",
    "By looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly. Your task here is to write the following code to:\n",
    "- Subtract the mean value of each feature from the dataset.\n",
    "- After subtracting the mean, additionally scale (divide) the feature values by their respective *standard deviations*.\n",
    "\n",
    "In Python, you can use the numpy function `np.mean(..)` to compute the mean. This function can directly be used on a $d$-dimensional dataset to compute a $d$-dimensional mean vector `mu` where each value `mu[j]` is the mean of the $j^{th}$ feature. This is done by setting the $2^{nd}$ argument `axis` of this function to `0`. For example, consider the following matrix `A` where each line corresponds to one data-point and each column corresponds to one feature:\n",
    "\n",
    "```python\n",
    "A = [[ 100,    10],\n",
    "     [ 30,     10],\n",
    "     [ 230,    25]]\n",
    "```\n",
    "\n",
    "In this case, `np.mean(A, axis=0)` will give `[120,   15]` where 120 is the mean of the 1st  column (1st feature) and 15 is the mean of the 2nd column (2nd feature). Another function `np.std(..)` exists to compute the standard deviation. The standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (usually, most data points will lie within the interval: mean $\\pm$ 2 standard_deviation).\n",
    "\n",
    "Once the features are normalized, you can do a scatter plot of the original dataset `X` (size of the house vs. number of bedrooms) and a scatter plot of the normalized dataset `X_normalized`. You will notice that the normalized dataset still have the same shape as the original one; the difference is that the new feature values have a similar scale and are centred arround the origin.\n",
    "\n",
    "**Implementation Note**: When normalizing the features, it is important to store the values used for normalization (the mean and the standard deviation used for the computations). Indeed, after learning the parameters of a model, we often want to predict the prices of houses we have not seen before. Given a new $x$ value (living room area and number of bedrooms), we must first normalize $x$ using the mean and standard deviation that we had previously computed from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\n- Do a scatter plot of the original dataset X\\n- Do a scatter plot of the normalized dataset X_normalized\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\"\"\" TODO:\n",
    "Complete the following code to compute a normalized version of X called: X_normalized\n",
    "\"\"\"\n",
    "# TODO: compute mu, the mean vector from X\n",
    "# TODO: compute std, the standard deviation vector from X\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "- Do a scatter plot of the original dataset X\n",
    "- Do a scatter plot of the normalized dataset X_normalized\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what you did in Lab2 Part B, you can simplify your implementation of linear regression by adding an additional first column to `X_normalized` with all the values of this column set to $1$. To do this you can re-use the function `add_all_ones_column(..)` defined in Lab2 Part B, which takes a matrix as argument and returns a new matrix with an additional first column (of ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nJust uncomment the following lines to create a matrix \\nX_normalized_new with an additional first column (of ones).\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Copy-past here the definition of the function add_all_ones_column(...) that \n",
    "you have see in Lab 2 (Part B).\n",
    "\"\"\"\n",
    "# definition of the function add_all_ones_column() here ...\n",
    "def add_all_ones_column(X):\n",
    "    n, d = X.shape # dimension of the matrix X (n lines, d columns)\n",
    "    XX = np.ones((n, d+1)) # new matrix of all ones with one additional column\n",
    "    XX[:, 1:] = X # set X starting from column 1 (keep only column 0 unchanged)\n",
    "    return XX\n",
    "\n",
    "\"\"\" TODO:\n",
    "Just uncomment the following lines to create a matrix \n",
    "X_normalized_new with an additional first column (of ones).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now ready to implement the linear regression using gradient descent (with more than one feature). In this multivariate case, you can further simply your implementation by writing the cost function in the following vectorized form:\n",
    "\n",
    "$$E(\\theta) = \\frac{1}{2n} (X \\theta - y)^T (X \\theta - y)$$\n",
    "\n",
    "$$\\text{where }\\quad\n",
    "X = \\begin{bmatrix}\n",
    "-- ~ {x^{(1)}}^T ~ -- \\\\ \n",
    "-- ~ {x^{(2)}}^T ~ -- \\\\ \n",
    "\\vdots \\\\ \n",
    "-- ~ {x^{(n)}}^T ~ --\n",
    "\\end{bmatrix}\n",
    "\\quad \\quad \\quad\n",
    "y = \\begin{bmatrix}\n",
    "y^{(1)} \\\\ \n",
    "y^{(2)} \\\\ \n",
    "\\vdots \\\\ \n",
    "y^{(n)} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The vectorized form of the gradient of $E(\\theta)$ is a vector denoted as $\\nabla E(\\theta)$ and defined follows:\n",
    "\n",
    "$$\\nabla E(\\theta) = \\left ( \\frac{\\partial E}{\\partial \\theta_0}, \\frac{\\partial E}{\\partial \\theta_1}, \\dots, \\frac{\\partial E}{\\partial \\theta_d} \\right ) = \\frac{1}{n} X^T (X \\theta - y)$$\n",
    "\n",
    "this is a **vector** where each $j^{th}$ value corresponds to $\\frac{\\partial E}{\\partial \\theta_j}$ (the derivative of the function $E$ with respect to the parameter $\\theta_j$)\n",
    "\n",
    "One your code is finished, you will get to try out different learning rates $\\alpha$ for the dataset and find a learning rate that converges quickly. To do so, you can plot the history of the cost $E(\\theta)$ with respect to the number of iterations at the end of your code.\n",
    "\n",
    "For example for alpha values of 0.01, 0.05 and 0.1, the plot should look like follows:\n",
    "<img src=\"imgs/costLab2C.png\" width=\"400px\" />\n",
    "\n",
    "If your learning rate is too large, $E(\\theta)$ can diverge and *blow up*, resulting in values which are too large for computer calculations. In these situations, Python will tend to return `NaN` or `inf` (NaN stands for \"*not a number*\" and is often caused by undefined operations that involve $-\\inf$ and $+\\inf$). If your value of $E(\\theta)$ increases or even blows up, adjust your learning rate and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" TODO: \n",
    "Write the cost function E using the vectorized form\n",
    "\"\"\"\n",
    "def E(theta, X, y):\n",
    "    return (X@theta- y).T @ (X@theta - y) / (2*len(y))\n",
    "\n",
    "\"\"\" TODO: \n",
    "Define the function grad_E (the gradient of E) using the vectorized form.\n",
    "This should return a vector of the same dimension as theta\n",
    "\"\"\"\n",
    "def grad_E(theta, X, y):\n",
    "    return X.T @ (X@theta - y) / len(y)\n",
    "\n",
    "\"\"\" TODO: \n",
    "Complete the definition of the function LinearRegressionWithGD(...) below\n",
    "Note: don't forget to call the functions E(..) and grad_E(..) with X_normalized_new instead of X\n",
    "\n",
    "The arguments of LinearRegressionWithGD(..) are:\n",
    "*** theta: vector of initial parameter values\n",
    "*** alpha: the learning rate (used by gradient descent)\n",
    "*** max_iterations: maximum number of iterations to perform\n",
    "*** epsilon: to stop iterating if the cost decreases by less than epsilon\n",
    "\n",
    "The function returns:\n",
    "*** errs: a list corresponding to the historical cost values\n",
    "*** theta: the final parameter values\n",
    "\"\"\"\n",
    "def LinearRegressionWithGD(theta, alpha, max_iterations, epsilon):\n",
    "    errs = []\n",
    "    \n",
    "    for itr in range(max_iterations):\n",
    "        mse = E(theta, X_normalized_new, y)\n",
    "        errs.append(mse)\n",
    "        \n",
    "        # TODO: take a gradient descent step to adapt the vector of parameters theta\n",
    "        # ...\n",
    "        \n",
    "        \n",
    "        # TODO: test if the cost decreases by less than epsilon (to stop iterating)\n",
    "        #if CONDITION:\n",
    "            #break\n",
    "    \n",
    "    return errs, theta\n",
    "\n",
    "\n",
    "\"\"\" TODO: \n",
    "Here you will call LinearRegressionWithGD(..) in a loop with different values of alpha, \n",
    "and plot the cost history (errs) returned by each call of LinearRegressionWithGD(..)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "theta_init = np.array([0, 0, 0])\n",
    "max_iterations = 200\n",
    "epsilon = 0.000000000001\n",
    "colors = [\"red\" , \"orange\" , \"blue\"]\n",
    "colors_index = 0\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.1]:\n",
    "    pass\n",
    "    # pass\n",
    "    # TODO: call LinearRegressionWithGD(...) using the current alpha, to get errs and theta\n",
    "    # ...\n",
    "\n",
    "    # ...\n",
    "    \n",
    "    # plot the errs using ax.plot(..)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once you have found a good $\\theta$ using gradient descent, use it to make a price prediction for a new house of 1650-square-foot with 3 bedrooms. **Note**: since the parameter vector $\\theta$ was learned using the normalized dataset, you will need to normalize the new data-point corresponding to this new house before predicting its price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nHINT: if you are not able to compute the dot product between x and theta, then \\nmake sure that the arrays have the same size. Did you forget something?\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use theta to predict the price of a 1650-square-foot house with 3 bedrooms\n",
    "Don't forget to normalize the feature values of this new house first.\n",
    "\"\"\"\n",
    "# Cretate a data-point x corresponding to the new house\n",
    "# Normalize the feature values of x\n",
    "# Use the vector of parameters theta to predict the price of x\n",
    "\n",
    "\"\"\"\n",
    "HINT: if you are not able to compute the dot product between x and theta, then \n",
    "make sure that the arrays have the same size. Did you forget something?\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Equation: Linear regression without gradient descent\n",
    "\n",
    "As you know from the lecture, the MSE cost function $E(\\theta)$ that we are trying to minimize is a convex function, and its derivative at the optimal $\\theta$ (that minimizes $E(\\theta)$) is equal to $0$. Therefore, to find the optimal $\\theta$, one can simply compute the derivative of $E(\\theta)$ with respect to $\\theta$, set it equal to $0$, and solve for $\\theta$.\n",
    "\n",
    "We have seen in the lecture that, by doing this, the closed-form solution is given as follows:\n",
    "$$\\theta = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "Using this formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \"*loop until convergence*\" like in gradient descent.\n",
    "\n",
    "You are asked to implement this equation to directly compute the best parameter vector $\\theta$ for the linear regression. In Python, you can use the `inv` function from `numpy.linalg.inv` to compute the inverse of a function.\n",
    "\n",
    "Remember that while you don't need to scale your features, we still need to add a column of 1's to the $X$ matrix to have an intercept term ($\\theta_0$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO: \\nCompute the optimal theta using new_X and y (without using gradient descent).\\nUse the normal equation shown above. You can use the function inv (imported above)\\nto compute the inverse of a matrix.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy.linalg import inv\n",
    "\n",
    "\"\"\" TODO: \n",
    "Use the function add_all_ones_column(..) to add a column of 1's to X. \n",
    "Let's call the returned dataset X_new.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" TODO: \n",
    "Compute the optimal theta using new_X and y (without using gradient descent).\n",
    "Use the normal equation shown above. You can use the function inv (imported above)\n",
    "to compute the inverse of a matrix.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once you have computed the optimal $\\theta$, use it to make a price prediction for the new house of 1650-square-foot with 3 bedrooms. Remeber that $\\theta$ was computed above based on the original dataset (without normalization); so, you do not need to normalize the feature values of the new house to make the prediction in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO: \\nUse theta to predict the price of a 1650-square-foot house with 3 bedrooms\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use theta to predict the price of a 1650-square-foot house with 3 bedrooms\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the previous formula does not require any feature normalization or scaling. However, you can still compute again the optimal $\\theta$ when using `X_normalized_new` instead of `new_X`.\n",
    "\n",
    "By doing this, you will be able to compare the $\\theta$ that you compute here with the one you got previously when you used gradient descent. The two parameter vectors should be quite similar (but not necessarily exatly the same)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO: \\nCompute the optimal theta using X_normalized_new and y (without using gradient descent). \\nUse the normal equation (shown previously).\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Compute the optimal theta using X_normalized_new and y (without using gradient descent). \n",
    "Use the normal equation (shown previously).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, now that you have computed the optimal $\\theta$ based on `X_normalized_new`, use it to make a price prediction for the new house of 1650-square-foot with 3 bedrooms. Do you need to normalize the feature values of the new house here? Remeber that $\\theta$ was computed here based on the normalized dataset.\n",
    "\n",
    "You should find that this predicted price similar to the price you predicted previsouly for the same house. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO: \\nUse theta to predict the price of a 1650-square-foot house with 3 bedrooms\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" TODO: \n",
    "Use theta to predict the price of a 1650-square-foot house with 3 bedrooms\n",
    "\"\"\"\n",
    "# Cretate a data-point x corresponding to the new house\n",
    "# Normalize the feature values of x\n",
    "# Use the vector of parameters theta to predict the price of x\n",
    "# print(\"prediction:\", pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
