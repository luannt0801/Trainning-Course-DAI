{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 (Part B): Classification with kNN\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "\n",
    "__IMPORTANT__ \n",
    "Please complete this Jupyter Notebook file and upload it to blackboard __before 13 February 2020__.\n",
    "</div>\n",
    "\n",
    "In this part of the Lab, you will implement the *k nearest neighbours* (kNN) classification method, and apply it to a dataset. Your task is to predict whether microchips from a fabrication plant passes quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly.\n",
    "\n",
    "Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine whether the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a *k nearest neighbours* classifier.\n",
    "\n",
    "## Loading the data\n",
    "We have a file `microchips-dataset.csv` which contains the dataset for our *nonlinear* classification problem. The first column corresponds to the result of \"microchip test 1\", the second column corresponds to the result of \"microchip test 2\", and the third column is the class-label indicating if the microchip has been accepted or rejected (1 = Accepted, 0 = Rejected).\n",
    "\n",
    "<img src=\"imgs/MicroshipDataLab3B.png\" />\n",
    "\n",
    "Complete the following Python code to load the dataset from the csv file into the variables `X` (input data matrix) and `y` (output class-labels). `X` should be a matrix with $n$ lines and $2$ columns (i.e. two feature) corresponding to \"microchip test 1\" and \"microchip test 2\". `y` should be a numpy array of $n$ elements.\n",
    "\n",
    "**Note**: You DO NOT need to add an additional column of all ones to the dataset as we are NOT using a linear model of the form $h_{\\theta}(x)={\\theta}^T x$ in this part of the Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nWrite the code to load the dataset from the `filename` into the variables X and y.\\nX should be a numpy array of n lines and 2 columns (the input data matrix).\\ny should be a numpy array of n elements (the outputs vector).\\nTry to do it by yourself and only check the code used in previous Labs if you are uncertain.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "\n",
    "filename = \"datasets/microchips-dataset.csv\"\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write the code to load the dataset from the `filename` into the variables X and y.\n",
    "X should be a numpy array of n lines and 2 columns (the input data matrix).\n",
    "y should be a numpy array of n elements (the outputs vector).\n",
    "Try to do it by yourself and only check the code used in previous Labs if you are uncertain.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data\n",
    "Similar to the previous part of the Lab, before starting to implement anything, it is always good to visualize the data if possible. Complete the following Python code so that it displays a figure like the one shown below. The two dimensions (features) correspond to the two tests results, and the class-labels are shown with different markers/colors.\n",
    "\n",
    "<img src=\"imgs/MicroshipScatterPlotLab3B.png\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nWrite code here to produce a scatter plot of the training \\ndataset like the one shown in the figure above.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "\"\"\" TODO:\n",
    "Write code here to produce a scatter plot of the training \n",
    "dataset like the one shown in the figure above.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side Note:** From the figure you can see that our dataset cannot be separated into positive (class 1) and negative (class 0) examples by a straight-line through the plot. Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary. If one still wants to use logistic regression, then one way to fit the data better is to create more features from each data-point by mapping the features into polynomial terms of $x_1$ and $x_2$ (e.g. $x_1^2$, $x_1 x_2$, etc ...). However, in this part of the Lab, we will use a kNN which is a nonlinear classifier.\n",
    "\n",
    "## Implementing the k Nearest Neighbours (kNN) classifier\n",
    "\n",
    "In the following code, you are asked to first implement the definition of the distance function `dist(u, v)` which computes the euclidean distance between two vectors $u \\in \\mathbb{R}^d$ and $v \\in \\mathbb{R}^d$. The euclidean distance between $u$ and $v$ is defined as: $\\left \\| u - v \\right \\| = \\sqrt{\\sum_{j=1}^{d} (u_i - u_v)^2}$. Note that this is simply the norm of the vector $u - v$, so you can either code it by yourself in pure Python, or make use of the numpy function `np.lianalg.norm(..)` which returns the norm of a given vector (or you can try both to see if you implemented it correctly).\n",
    "\n",
    "One you implement the distance function, you are asked to implement the definition of the function `prediction(x, X, y, k=5)`. This function takes as arguments a new data-point $x$ for which we want to predict the class-label, the training input data $X$, the output class-labels $y$, and a parameter $k$ corresponding to the number of nearest neighbours to use. The function should return the predicted class-label for $x$. To help you implement the function, you can follow the comments and make use of some predefined functions such as:\n",
    "- [numpy.argsort](https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html)\n",
    "- [collections.Counter](https://docs.python.org/dev/library/collections.html#collections.Counter.most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nTest your function prediction(x, X, y, k=5) on x = np.array([0, 0]); it \\nshould return the class-label 1 (i.e. accepted microship).\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\"\"\" TODO\n",
    "Implement the definition of the function dist(u, v) which\n",
    "computes the euclidean distance between two arrays u and v.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Implement the definition of the function prediction(x, X, y, k=5). You should return \n",
    "the predicted class-label for x, using the training dataset X, y, and k nearest neighbours.\n",
    "\"\"\"\n",
    "def prediction(x, X, y, k=5):\n",
    "    pass\n",
    "    # TODO: Compute the list of distances from x to each point in X\n",
    "    \n",
    "    # TODO: Get the list of indices sorted according to their corresponding distance\n",
    "    \n",
    "    # TODO: Take the class-labels corresponding to the first k indices (closest points to x)\n",
    "   \n",
    "    # TODO: The predicted class-label is the most common one among these class-labels\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Test your function prediction(x, X, y, k=5) on x = np.array([0, 0]); it \n",
    "should return the class-label 1 (i.e. accepted microship).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the decision boundary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, a function `plot_decision_boundary(func, X, y, k)` is provided to you. This fonction takes as a first argument the name of the prediction function (that you defined previously), and plots the decision boundary and the training dataset. You can read it if you want, but you DO NOT need to fully understand it. Your task here is to simply call the function `plot_decision_boundary(func, X, y, k)` a couple of times with different values of $k$, and see the difference in the decision boundary. Is the kNN decision boundary more complex when $k$ is smaller? It should be.\n",
    "\n",
    "*Note*: when the function is called, it can take some time (few seconds) before the plots are produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO:\\nCall here the function plot_decision_boundary(..) a couple of times with \\ndifferent values of k, and see the difference in the decision boundary. \\nNormally, the decision boundary looks more complex when k is smaller.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "# This function plots the decision boundary and the training dataset\n",
    "# You can read it if you want, but you don't need to fully understand it.\n",
    "def plot_decision_boundary(func, X, y, k):\n",
    "    print(\"Please wait. This might take few seconds to plot ...\")\n",
    "    min_x1, max_x1 = min(X[:, 0]) - 0.1, max(X[:, 0]) + 0.1\n",
    "    min_x2, max_x2 = min(X[:, 1]) - 0.1, max(X[:, 1]) + 0.1\n",
    "\n",
    "    plot_x1, plot_x2 = np.meshgrid(np.linspace(min_x1, max_x1, 50), np.linspace(min_x2, max_x2, 50))\n",
    "    points = np.c_[plot_x1.ravel(), plot_x2.ravel()]\n",
    "    preds = np.array([ func(x, X, y, k) for x in points ])\n",
    "    preds = preds.reshape(plot_x1.shape)\n",
    "\n",
    "    X0 = X[y==0]\n",
    "    X1 = X[y==1]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pcolormesh(plot_x1, plot_x2, preds, cmap=ListedColormap(['#FFAAAA', '#AAAAFF']))\n",
    "    ax.scatter(X0[:, 0], X0[:, 1], color=\"red\", label=\"Rejected\")\n",
    "    ax.scatter(X1[:, 0], X1[:, 1], color=\"blue\", label=\"Accepted\")\n",
    "    ax.set_xlabel(\"Microship Test 1\")\n",
    "    ax.set_xlabel(\"Microship Test 2\")\n",
    "    ax.set_title(\"Decision boundary with k = {}\".format(k))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\"\"\" TODO:\n",
    "Call here the function plot_decision_boundary(..) a couple of times with \n",
    "different values of k, and see the difference in the decision boundary. \n",
    "Normally, the decision boundary looks more complex when k is smaller.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the kNN classifier\n",
    "One way to evaluate the quality of our classifier is to see how well it predicts on our training set. In this part, your task is to complete the Python code below to report the training accuracy of your classifier by computing the percentage of examples for which you correctly predicted the class-label.\n",
    "\n",
    "*Note*: We will see later in the course that computing the ***training** accuracy* is NOT a good way to evaluate the quality of your machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8050847457627118\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO:\n",
    "Predict the class-labels of the data-points in the training set by calling the function \n",
    "prediction(..) on each data-point in X. Then, compute the classification accuracy by comparing \n",
    "the predicted class-labels with the actual (true) class-labels y. Use a value of k = 15.\n",
    "\"\"\"\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "y_pred = np.array([prediction(x , X , y , 15 ) for x in X])\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: Weighted k Nearest Neighbours (kNN) classifier\n",
    "This task is optional. Your task here is to re-define your previous function `prediction(x, X, y, k=5)` so that it corresponds to the weighted kNN. You can define the weights as the inverse of the distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TODO\\nTest the function prediction_weighted(x, X, y, k) by calling it. Then, \\ncall the function plot_decision_boundary(..) a couple of times with \\ndifferent values of k, and see the difference in the decision boundary.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" TODO:\n",
    "Implement the definition of the function prediction_weighted(x, X, y, k=5). You should return \n",
    "the predicted class-label for x, using the training dataset X, y, and k.\n",
    "\"\"\"\n",
    "def prediction_weighted(x, X, y, k=5):\n",
    "    # calculate distances \n",
    "    distances = []\n",
    "    for point in X :\n",
    "        distances.append(dist(x , point))\n",
    "    weighted = [] \n",
    "    for distance in distances :\n",
    "        weighted.append((1/distance)**2)\n",
    "    \n",
    "    distances = np.array(distances)\n",
    "    weighted = np.array(weighted)\n",
    "    sorted_indices = np.argsort(distances)\n",
    "\n",
    "    # prediction \n",
    "    k_class_labels = y[sorted_indices][:k]\n",
    "    k_weights = weighted[sorted_indices][:k]\n",
    "    w = [0, 0]\n",
    "\n",
    "    for i in range(k):\n",
    "        w[int(k_class_labels[i])] += k_weights[i]\n",
    "    \n",
    "    # return value \n",
    "    return 0 if w[0]>w[1] else 1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" TODO\n",
    "Test the function prediction_weighted(x, X, y, k) by calling it. Then, \n",
    "call the function plot_decision_boundary(..) a couple of times with \n",
    "different values of k, and see the difference in the decision boundary.\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
